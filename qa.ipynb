{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import  Pinecone\n",
    "import pinecone   \n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI   \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import itertools\n",
    "import ast \n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "\n",
    "pinecone.init(\n",
    "\tapi_key='4c069a19-af2e-492d-b9a8-5fe09789a431',      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index = pinecone.Index('python-index')\n",
    "indexName = \"python-index\"\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model='text-embedding-ada-002',\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def prepare_embedding_and_metadata(df):\n",
    "    \"\"\"Prepares embedding data and combines with metadata.\"\"\"\n",
    "    \n",
    "    df['embedding'] = df['embedding'].apply(ast.literal_eval)\n",
    "    embeddings = df['embedding'].tolist()\n",
    "    \n",
    "    # Assuming you have metadata columns like 'title', 'url', etc. Adjust as necessary.\n",
    "    # This will create a list of metadata dictionaries for each row.\n",
    "    metadatas = df[[\"Clothing ID\", \"Age\", \"Title\", \"Review Text\", \"Rating\", \"Recommended IND\", \"Positive Feedback Count\", \"Division Name\", \"Department Name\", \"Class Name\",'combined',\"Product Name\"]].to_dict(orient='records')\n",
    "\n",
    "    combined_data = []\n",
    "    for i, (embedding, metadata) in enumerate(zip(embeddings, metadatas)):\n",
    "        data = {\n",
    "            'id': str(i),\n",
    "            'embedding': embedding,\n",
    "            **metadata\n",
    "        }\n",
    "        combined_data.append(data)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def upload_chunk_to_index(chunk_data):\n",
    "    \"\"\"Uploads a chunk of data to the index.\"\"\"\n",
    "    \n",
    "    # Extracting ids, embeddings, and metadata from the chunk_data\n",
    "    ids = [item['id'] for item in chunk_data]\n",
    "    embeds = [item['embedding'] for item in chunk_data]\n",
    "    \n",
    "    # Since the metadata is everything in the item except for 'id' and 'embedding', \n",
    "    # we'll create metadata by excluding these fields.\n",
    "    metadatas = [{key: val for key, val in item.items() if key not in ['id', 'embedding']} for item in chunk_data]\n",
    "    \n",
    "    # Using zip to combine ids, embeds, and metadatas in the expected format\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "def uploadData():\n",
    "    inputDatapath = \"embeddings.csv\"\n",
    "    df = pd.read_csv(inputDatapath)\n",
    "    \n",
    "    combined_data = prepare_embedding_and_metadata(df)\n",
    "    \n",
    "    # Use tqdm to show progress when processing the chunks\n",
    "    for chunk in tqdm(chunks(combined_data, 100)):\n",
    "        upload_chunk_to_index(chunk)\n",
    "\n",
    "    print(\"Done uploading data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a047aa7ed240a09f34d0093347c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done uploading data\n"
     ]
    }
   ],
   "source": [
    "uploadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "text_field = \"combined\"\n",
    "\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average age of the reviewers is 52.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"What is the average age of the reviewers? each reviewer corresponds to a review\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 47'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY), vectorstore.as_retriever(), memory=memory)\n",
    "\n",
    "query = \"What is the average age of the reviewers? each reviewer corresponds to a review\"\n",
    "result = qa({\"question\": query})\n",
    "\n",
    "result[\"answer\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 45'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Based on the previous answer, what is the age of the reviewer who gave the highest rating?\"\n",
    "result = qa({\"question\": query})\n",
    "result[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
